# cs231n  随笔（以图片记录为主）

![image-20221119233056040](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119233056040.png)

![image-20221119233109477](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119233109477.png)

![image-20221119233849736](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119233849736.png)

![image-20221119234202499](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119234202499.png)

![image-20221119235625050](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119235625050.png)

![image-20221120000053728](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120000053728.png)

![image-20221120232101328](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120232101328.png)

![image-20221120234240789](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120234240789.png)

例如x为5000 * 32 * 32 * 3的数组，则x.shape[0]=5000 , x.shape[1]=32 , x.shape[2]=32 , x.shape[3]=3

![image-20221120234746564](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120234746564.png)

![image-20221121090454510](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121090454510.png)

```python
a=a.reshape(a.shape[0],-1)
a=np.reshape(a,(a.shape[0],-1))
```

![image-20221121093514024](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121093514024.png)

![image-20221121131636527](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121131636527.png)

![image-20221121133817348](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121133817348.png)

![image-20221121134207024](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121134207024.png)

对于二维数组而言，axis=0代表对列操作，axis=1代表对行操作

![image-20221121142437963](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121142437963.png)

np.sum之后变为一维数组，需要加上参数 keepdims=True

![image-20221121143510790](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121143510790.png)

## 11.21

![image-20221123235232620](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221123235232620.png)

![image-20221123235851274](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221123235851274.png)

np.split使得数组分割为列表，np.concatenate使得列表拼接为数组

## 12.2

![image-20221202172231173](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221202172231173.png)

结果是列数加1

![image-20221203153030134](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221203153030134.png)

![image-20221203153043996](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221203153043996.png)

## 12.6

向前传递的梯度等于上游传下来的梯度值乘以**自身**在该处的导数值

SGD：随机梯度下降

![image-20221206203420151](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221206203420151.png)

np.std()默认是有偏的，即分母除以n，要变成无偏的，将ddof设置为1

同样的,np.var()亦是有偏的

## 1.23

![image-20230123161424037](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230123161424037.png)

权重矩阵的初始化至关重要：

- 过小时，由于不断相乘，梯度衰减到0，无法学习
- 过大时，梯度爆炸，神经元死亡，无法学习
- 网络越深，对weight_scale的变化越敏感（每一层都相乘）



### SGD + Momentum

通常指 Nesterov Momentum

![image-20230123164324710](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230123164324710.png)

 代码如下：

```python
v = config["momentum"]*v - config["learning_rate"]*dw
next_w = w+v
```



### RMSprop

```python
config['cache'] = config['decay_rate']*config['cache'] + (1-config['decay_rate'])*dw*dw
next_w = w - config['learning_rate']*dw/(np.sqrt(config['cache']) + config['epsilon'])
```



### Adam

```python
config['t'] += 1
config['m'] = config['beta1']*config['m'] + (1-config['beta1'])*dw
config['v'] = config['beta2']*config['v'] + (1-config['beta2'])*dw*dw
mt = config['m']/(1-config['beta1']**config['t']) #注意这里无偏化不更新config['m']
vt = config['v']/(1-config['beta2']**config['t'])
next_w = w - config['learning_rate']*mt/(np.sqrt(vt) + config['epsilon'])
pass
```



## 1.25  BaychNormalization

![image-20230125150502311](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230125150502311.png)

反向传播用于训练过程中，因此这时的$\mu$一定是样本自身的均值而非训练后生成的用于测试的均值，因此在求导时需要考虑这一项（方差$\sigma$同理）

- 矩阵求导就是对应项求导（$\frac{dL}{dx}$，求对某个$x_{ij}$的导数 ：==>$f$中所有含$x_{ij}$的项如$f_{mn}$,其贡献为$\frac{dL}{df_{mn}}\cdot\frac{df_{mn}}{dx_{ij}}$,对所有这样的$f_{mn}$求和，即为$\frac{dL}{dx_{ij}}$）
- 上游导数对应的值乘以本地导数值再求和
- 注意矩阵的size，从二维到一维一般是求和(对于广播后相乘的矩阵都是如此)
- 原矩阵广播==>导数矩阵也是广播，故原矩阵求和降维则导数矩阵求和降维

