# cs231n  随笔（以图片记录为主）

![image-20221119233056040](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119233056040.png)

![image-20221119233109477](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119233109477.png)

![image-20221119233849736](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119233849736.png)

![image-20221119234202499](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119234202499.png)

![image-20221119235625050](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221119235625050.png)

![image-20221120000053728](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120000053728.png)

![image-20221120232101328](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120232101328.png)

![image-20221120234240789](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120234240789.png)

例如x为5000 * 32 * 32 * 3的数组，则x.shape[0]=5000 , x.shape[1]=32 , x.shape[2]=32 , x.shape[3]=3

![image-20221120234746564](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221120234746564.png)

![image-20221121090454510](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121090454510.png)

```python
a=a.reshape(a.shape[0],-1)
a=np.reshape(a,(a.shape[0],-1))
```

![image-20221121093514024](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121093514024.png)

![image-20221121131636527](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121131636527.png)

![image-20221121133817348](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121133817348.png)

![image-20221121134207024](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121134207024.png)

对于二维数组而言，axis=0代表对列操作，axis=1代表对行操作

![image-20221121142437963](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121142437963.png)

np.sum之后变为一维数组，需要加上参数 keepdims=True

![image-20221121143510790](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221121143510790.png)

## 11.21

![image-20221123235232620](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221123235232620.png)

![image-20221123235851274](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221123235851274.png)

np.split使得数组分割为列表，np.concatenate使得列表拼接为数组

## 12.2

![image-20221202172231173](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221202172231173.png)

结果是列数加1

![image-20221203153030134](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221203153030134.png)

![image-20221203153043996](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221203153043996.png)

## 12.6

向前传递的梯度等于上游传下来的梯度值乘以**自身**在该处的导数值

SGD：随机梯度下降

![image-20221206203420151](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20221206203420151.png)

np.std()默认是有偏的，即分母除以n，要变成无偏的，将ddof设置为1

同样的,np.var()亦是有偏的

## 1.23

![image-20230123161424037](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230123161424037.png)

权重矩阵的初始化至关重要：

- 过小时，由于不断相乘，梯度衰减到0，无法学习
- 过大时，梯度爆炸，神经元死亡，无法学习
- 网络越深，对weight_scale的变化越敏感（每一层都相乘）



### SGD + Momentum

通常指 Nesterov Momentum

![image-20230123164324710](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230123164324710.png)

 代码如下：

```python
v = config["momentum"]*v - config["learning_rate"]*dw
next_w = w+v
```



### RMSprop

```python
config['cache'] = config['decay_rate']*config['cache'] + (1-config['decay_rate'])*dw*dw
next_w = w - config['learning_rate']*dw/(np.sqrt(config['cache']) + config['epsilon'])
```



### Adam

```python
config['t'] += 1
config['m'] = config['beta1']*config['m'] + (1-config['beta1'])*dw
config['v'] = config['beta2']*config['v'] + (1-config['beta2'])*dw*dw
mt = config['m']/(1-config['beta1']**config['t']) #注意这里无偏化不更新config['m']
vt = config['v']/(1-config['beta2']**config['t'])
next_w = w - config['learning_rate']*mt/(np.sqrt(vt) + config['epsilon'])
pass
```



## 1.25  BaychNormalization

![image-20230125150502311](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230125150502311.png)

反向传播用于训练过程中，因此这时的$\mu$一定是样本自身的均值而非训练后生成的用于测试的均值，因此在求导时需要考虑这一项（方差$\sigma$同理）

- 矩阵求导就是对应项求导（$\frac{dL}{dx}$，求对某个$x_{ij}$的导数 ：==>$f$中所有含$x_{ij}$的项如$f_{mn}$,其贡献为$\frac{dL}{df_{mn}}\cdot\frac{df_{mn}}{dx_{ij}}$,对所有这样的$f_{mn}$求和，即为$\frac{dL}{dx_{ij}}$）
- 上游导数对应的值乘以本地导数值再求和
- 注意矩阵的size，从二维到一维一般是求和(对于广播后相乘的矩阵都是如此)
- 原矩阵广播==>导数矩阵也是广播，故原矩阵求和降维则导数矩阵求和降维



## 1.26  layer normalization

bn是对所有图像的某一个像素做平均，ln是对一个图像（某一通道）的所有像素做平均，即对特征做均值化

![img](https://img-blog.csdnimg.cn/20190208203843638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTg4MDU3OQ==,size_16,color_FFFFFF,t_70)

注意即为一个转置关系

## 1.27  bn梯度推导

![image-20230130143025281](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230130143025281.png)

![image-20230130143037813](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230130143037813.png)

代码如下：

```python
out_norm, gamma, std = cache
dbeta = np.sum(dout, axis=0)
dgamma = np.sum(dout*out_norm, axis=0)
dout_norm = dout*gamma
N,D = dout.shape
# dvar = np.sum(dout_norm*(-0.5)*out_norm/std**2, axis=0)
# dmu = -np.sum(dout_norm/std, axis=0)
# dx = dout_norm/std + dvar*std*out_norm*2/N + dmu/N
dx = (dout - (dgamma*out_norm + dbeta)/N)*gamma/std
```



## 1.27  之前的笔记

![image-20230130152652334](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230130152652334.png)

![image-20230130152703137](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230130152703137.png)

![image-20230130152710920](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230130152710920.png)

## 1.27  dropout

![image-20230130170525481](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230130170525481.png)

## 2.1  GPU

GPU存在于显卡中，显卡包括了GPU和显存等

GPU适合高度的并发计算（多核）

CUDA是NVIDIA基于GPU运算的编程语言（类C）

Numpy只能运行于CPU

### TensorFlow

首先定义变量及计算图，无需对变量赋值，计算图只是声明了一些逻辑，若要求梯度，会在计算图中加入一些额外的辅助逻辑，但始终没有真正的计算

定义完成后进入有个Tensorflow的session,在这个session中具体为计算图中的变量赋值，并执行计算

![image-20230201200251869](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230201200251869.png)

如果将w1和w2在计算图中作为占位符，训练过程中会不断在CPU和GPU之间传输数据，耗时且浪费资源(numpy只能在CPU上运行)，改进想法，将w1和w2作为变量存储在计算图中

注意此时需要初始化

![image-20230201200839071](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230201200839071.png)

更新操作也应体现在计算图中

## 2.5  cnn

![image-20230205170559189](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20230205170559189.png)

